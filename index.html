<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script id="MathJax-script" type="text/javascript" async
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <title>PCL</title>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]},
            "HTML-CSS": { availableFonts: ["TeX"] },
            tex: { 
                extensions: ["AMSmath.js", "AMSsymbols.js", "boldsymbol.js", "mhchem.js"]
            }
        });
    </script>
  
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body {
            font-family: 'Georgia', serif;
            line-height: 1.6;
            background-color: #f5f5f5;
            color: #333;
        }
        img {
            max-width: 100%;
            max-height: 100%;
            height: auto;
            width: auto;
        }

        .roman-numeral {
        font-family: 'Times New Roman', Times, serif; /* 使用罗马字体 */
        font-size: 16px; /* 设置字体大小 */
        }
        .figure-container {
            display: flex;
            justify-content: space-around;
            align-items: center; /* Center align items vertically */
            margin: 20px 0;
        }
        header {
            background-color: white;
            padding: 30px 0;
            color: white;
            text-align: center;
        }
        h1 {
            color: black;
            font-size: 33px;
            line-height: 1.2;
            margin-bottom: 20px;
        }
        p {
            color: #555;
            font-size: 16px;
            margin-bottom: 10px;
        }
        section {
            margin: 40px 0;
            padding: 20px;
            background-color: white;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
        }
        h2 {
            color: #007BFF;
            border-bottom: 2px solid #007BFF;
            padding-bottom: 5px;
            margin-bottom: 20px;
        }
        /* h3 {
            color: #ea7909;
            font-size: 20px;
            margin-bottom: 20px;
        }  */
        ul {
            list-style-type: square;
            margin-left: 20px;
        }
        code {
            font-family: 'Courier New', Courier, monospace;
            background-color: #f9f9f9;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 3px;
            display: block;
            overflow-x: auto;
            margin-bottom: 20px;
        }
        figure {
            margin: 20px 0;
        }
        figcaption {
            color: #777;
            font-style: italic;
            text-align: center; /* 设置对齐方式为居中 */
    /* 或者使用其他值，如 left（左对齐）、right（右对齐） */
            font-size: 16px;
        }
        footer {
            background-color: white;
            padding: 20px 0;
            color: white;
            text-align: center;
        }
    </style>
</head>
<body>
    <header>
        <h1 style="white-space: pre-line;">
            Personalized Clustering via Targeted Representation Learning
        </h1>
        <div class="author">
            Anonymous Author
        </div>
    </header>

    <div class="container">
        <section id="abstract">
            <h2>1. Abstract</h2>
            <!-- <span style="color: red;">red</span>  -->
            <p> Clustering traditionally aims to reveal a natural grouping structure model from unlabeled data. However, this model may not always be ideal for users' preference. In this paper, we propose a personalized clustering method which explicitly performs targeted representation learning by interacting with users via modicum task information (e.g., must-link or cannot-link pairs) to guide the clustering orientation. First, we query users with the the most informative pairs, i.e., those pairs most hard to cluster and those most easy to miscluster, to facilitate the representation learning in terms of the clustering preference. And then, by exploiting attention mechanism the targeted representation is learned and augmented. By leveraging the targeted representation and constraint clustering loss as well, personalized clustering is obtained. Theoretically, we verify that the risk of personalized clustering is tightly bounded, that guarantees active queries to users do mitigate the clustering risk. Experimentally, extensive results show that our method performs well across different clustering tasks and datasets, even with a limited number of queries.
                </p>
        </section>

        <section id="problem">
            <h2>2. Problem Defination</h2>
            <ul>
                <li> <span style="color: green;"><u>(Section 1)</u></span> </span>In the domain of image clustering, varying tasks often entail distinct clustering objectives. It is evident that unsupervised methods may not consistently excel across all clustering dimensions. This paper introduces a weakly supervised clustering approach, augmented by active learning, to adaptively address clustering tasks aligned with different clustering orientations, as illustrated in Figure 1.</li>
                <div class="img" style="text-align:center">
                    <img class="img_responsive" src="./assert/Figure1.png" alt="tasked class" style="margin:auto;max-width:40%">
                    <figcaption>Figure 1: The diversity of clustering orientation. Different tasks have different orientations for feature learning and image clustering.</figcaption>
                </div>
                <br>
                <li> <span style="color: green;"><u>(Subsection 3.1</span>)</u></span> For the image datasets, data is initially defined by generating original positive instance pairs through data augmentation techniques. Subsequently, based on the clustering results from each training iteration, the most informative sample pairs are selected for query. The clustering orientation is then learned from the existing positive and negative instance pairs. Ultimately, this process yields clustering results tailored to meet the specific objectives of the task at hand.</span>
                </li>
            </ul>
            
        </section>

        <section id="framework">
            <h2>3.Illustration of Our Framework</h2>
            <p>
                Our method mentioned above is presented in <span style="color: green;"><u> Section 3.2 & Section 3.3 & Appendix A</span> </u></span>. A deep neural network creates representations from two random augmentations of the data. By assessing the position of images in the feature space, the framework selects the most informative sample pairs to guide the training orientation of the model. The model is then retrained by querying whether these pairs are must-link or cannot-link. This approach allows the final model to concentrate on features relevant to the desired clustering orientation, resulting in accurate clustering outcomes. Our framework can be illustrated in Figure 2.
            </p>
            <div class="img" style="text-align:center">
                <img class="img_responsive" src="./assert/Figure2.png" alt="framework" style="margin:auto;max-width:90%">
                <figcaption>Figure 2: The framework of Personalized Clustering.</figcaption>
            </div>
        </section>

        <section id="Theory">
            <h2>4. Theoretical Analysis</h2>
            <p> We propose a theoretical analysis to verify effectiveness of active query in constraint clustering. Simultaneously, the tight upper bound of generalization risk of PCL is given as well.</p>
            <ul>
                <li><span style="color: green;"><u>(<span class="roman-numeral">Subsection 3.4 & Appendix C</span>)</u></span> <span style="color: red;"><strong>Definition 4.1 (Unsupervised Loss.)</strong></span> 
                <br>
                We need to measure the effect of clustering by an unsupervised loss. If we can query $q$ times, the overall unsupervised loss is defined as:
                    $$
                    \hat{\mathcal{L}}_q(f) := \frac{1}{2N}\sum_{i=1}^{2N}\ell_i^{'}(f).
                    $$
                    and $\hat{f}_{q}$ is defined as $\arg \min_{f \in \mathcal{F}} \hat{\mathcal{L}}_{q}(f)$,$\quad q$ is the total num of query. The unsupervised population loss after $q$ queries is defined as:
                    $$
                        \mathcal{L}_q(f) := \mathbb{E}_{x \sim \mathcal{X}}[\hat{\mathcal{L}}_q(f)],
                    $$
                    and $f^{*}_{q}$ is defined as $\arg \min_{f \in \mathcal{F}} \mathcal{L}_{q}(f)$.
                <br>
                <li><span style="color: green;"><u>(<span class="roman-numeral">Subsection 3.4 & Appendix C</span>)</u></span> <span style="color: red;"><strong>Theorem 4.1 (Excess Risk)</strong></span> 
                <br>
                    Theorem 3.1 Suppose we can completely query each sample. We use $Q$ as the total number of queries. With probability at least $1-\delta$,
                    $$
                        |\mathcal{\hat{L}}_{Q}(\hat{f}_{Q}) - \mathcal{L}_{Q}(f^{*}_{Q}) | \leq \mathcal{O}(\frac{\eta \mathcal{R}_{\mathcal{S}}(\mathcal{F})}{N} + \sqrt{\frac{\log\frac{1}{\delta}}{N}}).
                    $$
                    where $\mathcal{R}_{\mathcal{S}}(\mathcal{F})$ is the Rademacher complexity of $\mathcal{F}$ with respect to training set $\mathcal{S}$, and $\eta$ is the Lipschitz constant.
                    
                    Theorem 3.1 indicates that $\mathcal{L}_{Q}(f^{*}_{Q})$ can be well bounded if the trained model $\hat{f}_{Q}$ is good.
                    However, due to the limited queries (less than $Q$), we cannot calculate $\mathcal{\hat{L}}_{Q}(\hat{f}_{Q})$. As a compromise, can we get closer to it?
                <br>
                <span style="color: green;"><u>(<span class="roman-numeral">Subsection 3.4 & Appendix C</span>)</u></span> <span style="color: red;"><strong>Theorem 3.2 (Queries Reduce the Gap)</strong></span>
                <br>
                    We only consider adding one query. If $q \ll N$:
                    $$
                        |\mathcal{\hat{L}}_{q}(\hat{f}_{q}) - \mathcal{\hat{L}}_{Q}(\hat{f}_{Q})| \geq |\mathcal{\hat{L}}_{q+1}(\hat{f}_{q+1}) - \mathcal{\hat{L}}_{Q}(\hat{f}_{Q})|.
                    $$
                    
                    This just matches the process of algorithm: 
                    we initially make a new query request based on the existing model $\hat{f}_{q}$, 
                    followed by updating $\mathcal{\hat{L}}_{q}$ to $\mathcal{\hat{L}}_{q+1}$ based on the query results, 
                    and finally optimizing the loss to get $\hat{f}_{q+1}$. 
                    Theorem 3.2 guarantees the gap between the loss of the current model and the complete queries loss will be reduced after each query. Now we show that our strategy is indeed effective.
                <br>
                <span style="color: green;"><u>(<span class="roman-numeral">Subsection 3.4 & Appendix C</span>)</u></span> <span style="color: red;"><strong>Theorem 3.3 (Query Strategy Is Effective)</strong></span>
                <br>
                    Suppose $q \ll N$, the sample pair $(\mathbf x_i, \mathbf x_j)$ is queried, and the result is cannot-link,  we have:
                    $$
                        |\mathcal{\hat{L}}_{q}(f) - \mathcal{\hat{L}}_{q+1}(f)| \propto  \mathcal \exp \left(\mathcal{S}_{up}(\mathbf x_i,\mathbf x_j)\right),
                    $$
                    and if the result is must-link, we have:
                    $$
                        |\mathcal{\hat{L}}_{q}(f) - \mathcal{\hat{L}}_{q+1}(f)| \propto \mathcal S_{up}(\mathbf x_i,\mathbf x_j).
                    $$
                    
                    Since we want to reduce the gap in Theorem 3.2, we should equivalently maximize $|\mathcal{\hat{L}}_{q}(f) - \mathcal{\hat{L}}_{q+1}(f)|$ and then train the model according to $\mathcal{\hat{L}}_{q+1}$.
                    Theorem 3.3 states that our query strategy is effective in this sense, 
                    because when $q$ is small, $\mathcal{S}_{up}$ matters more in the scoring function.
                    Besides, we have added $\mathcal{S}_{hp}$ in the score and weight it more if $q$ is larger, making our strategy more robust and efficient. 
                    The following experiments have proved the effectiveness in practice.
                <br>
                </li>
            </ul>
        </section>

        <section id="experiment-results">
            <h2>5. Experiments</h2>
            <ul>
                <li>
                    <span style="color: green;"><u>(Subsection 4.1 & Appendix B)</u></span> <span style="color: red;"> <strong>Datasets.</strong></span> Our experiments are conducted on three datasets (CIFAR10-2, CIFAR100-4, Imagenet10-2) with two different orientations, encompassing three benchmark datasets (CIFAR-10, CIFAR-100, Imagenet-10), along with an artificial cross-dataset. The details of the datasets are as follows.

                    <div class="img" style="text-align:center">
                        <img class="img_responsive" src="./assert/Figure5.png" alt="dataset1" style="margin:auto;max-width:100%">
                        <figcaption>Table 1: A summary of the datasets. </figcaption>
                        <!-- The count of categories in labeled and unlabeled data is indicated as ``Categories''. The terms ``Num.'' indicate the count of labeled instances. For unlabeled instances, ``T'' and ``U'' represent the amount of instances from target and unknown categories, respectively, under varying mismatch proportions. -->
                    </div>

                    Figure 3 shows an example of two clustering orientations from the same original dataset. The details of creating our target dataset can be found at <span style="color: green;"><u>(Appendix B)</u></span>

                    <div class="img" style="text-align:center">
                        <img class="img_responsive" src="./assert/Figure6.png" alt="dataset2" style="margin:auto;max-width:80%">
                        <figcaption>Figure 3: The default and personalized orientation designed artificially. </figcaption>
                        <!-- The terms ``Num.'' and ``Categories'' are employed to denote the count and categories, respectively, for labeled instances. In the case of unlabeled instances, the number of instances from target and unknown categories is represented as ``T'' and ``U'', respectively, under varying mismatch proportions. The unknown categories in the realistic dataset consist of two components: the noise from 5 target categories ($\text{Noise}_5$) and the noise from the remaining 95 categories ($\text{Noise}_{95}$). -->
                    </div>
                </li>
                <li>
                    <span style="color: green;"><u>(<span class="roman-numeral">Subsection 4.2 </span>)</u></span> <span style="color: red;"> <strong>Partial Experimental Results.</strong></span> 
                    <img class="img_responsive" src="./assert/Figure7.png" alt="cifar100" style="margin:auto;max-width:90%">
                        <figcaption>Table 2: The clustering performance under the default orientation, on three object image benchmarks. The default orientation completely follows the tendency of deep clustering. The final results are shown in boldface. </figcaption>
                    <img class="img_responsive" src="./assert/Figure8.png" alt="cifar100" style="margin:auto;max-width:90%">
                        <figcaption>Table 3: The clustering performance under the personalized orientation, on three object image benchmarks. The personalized target orientation is artificially designed opposite to the default one. The final results are shown in boldface. </figcaption>
                </li>

                <li>
                    <span style="color: green;"><u>(<span class="roman-numeral">Subsection 4.3</span>)</u></span> <span style="color: red;"> <strong>Visualization Results.</strong></span> 
                    <div class="img" style="text-align:center">

                    <p style="text-align:left">Subfigure (a) depicts that the instance points before clustering is chaos.  Subfigure (b) depicts that the yellow and orange instance points cluster together, while in subfigure (c) the dark blue and dark orange dots are clustered together. The above facts show that it is workable for PCL to cluster according to the demand of tasks.
                    </p>
                        <img class="img_responsive" src="./assert/Figure9.png" alt="visual" style="margin:auto;max-width:90%">
                        <figcaption>Figure 4: The distribution of instances in CIFAR10-2 after clustering. </figcaption>
                    </div>
                </li>
                <li>
                    <span style="color: green;"><u>(<span class="roman-numeral">Subsection 4.3</span>)</u></span> <span style="color: red;"> <strong>Ablation Studies.</strong></span> 
                    <p>
                        We performed two ablation analysis by removing components of the query strategy and cross-attention module.
                    </p>
                    <div class="figure-container">
                        <figure>
                            <img class="img_ablation_query" src="./assert/Figure10.png" style="margin:center;max-width:70%">
                            <figcaption>Table 4:Effect of query strategy in personlized clustering on CIFAR100-4</figcaption>
                        </figure>
                        <figure>
                            <img class="img_ablation_attention" src="./assert/Figure11.png" style="margin:center;max-width:70%">
                            <figcaption>Table 5:Effect of Cross-Attention Module in personlized clustering on CIFAR100-4</figcaption>
                        </figure>
                    </div>
                </li>
            </ul>
        </section>

        <section id="controbution">
            <h2>6. Contribution</h2>
            <ul>
                <li>
                    We propose a novel model, called PCL, for personalized clustering task, which leverages active query to control the targeted representation learning.
                </li>
                
                <li>
                    A theoretical analysis on clustering is conducted to verify effectiveness of active query in constraint clustering. Simultaneously, the tight upper bound of generalization risk of PCL is given as well.
                </li>
        
                <li>
                    Extensive experiments show that our model outperforms four deep clustering approaches, three semi-supervised clustering approaches, and two active clustering approaches on three image datasets. Our active query strategy also performs well compared to other methods.
                </li>
               
            </ul>
        </section>

        <section id="code">
            <h2>7. Code</h2>
            <p>Please access our code through an anonymous link: <a href="https://anonymous.4open.science/r/PCL-B98C" target="_blank">PCL.</a>
            </p>
        </section>
    </div>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

</body>
</html>
